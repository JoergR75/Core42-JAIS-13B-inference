# ๐ Jais 13B Inference on AMD ROCm

This Python script provides a simple interface to run **JAIS 13B** language model inference on AMD GPUs using **ROCm** and Hugging Face Transformers. It allows generating text from custom prompts with configurable sampling parameters.

## Features

- Load and run the `core42/jais-13b-chat` model from Hugging Face.
- Supports prompts in any language.
- Configurable `max_new_tokens`, `temperature`, and `top_p` for text generation.
- Optional chat template formatting.
- Automatic device selection between ROCm-enabled GPU (`cuda`) and CPU.

## Requirements

- ROCm 6.4.2+ (Link to the automated script: https://github.com/JoergR75/ROCm-6.4.2-deployment-on-RDNA4)
- Python 3.12+
- PyTorch 2.9.0 with ROCm support (2.9.0.dev20250729+rocm6.4)
- Transformers library
- Hugging Face account and API token
- Sufficient GPU memory (~24โ32 GB VRAM recommended for 13B model)
- Tested on 1x Radeon AI PRO R9700 (32GB), 2x Radeon RX 9070 (16GB) and INSTINCT MI210 (64GB)

## Usage

Run the script with an Arabic prompt:โก make sure to use your HuggingFace token โก

```bash
python3 jais_13b_inference_test.py \
  --model core42/jais-13b-chat \
  --prompt "ุงูุชุจ ููุฑุฉ ูุตูุฑุฉ ุนู ุฃูููุฉ ุงูุจูุงูุงุช ูู ุงูุฑุนุงูุฉ ุงูุตุญูุฉ." \
  --hf_token YOUR_HF_TOKEN
```
=== Generated Text ===

ุงูุชุจ ููุฑุฉ ูุตูุฑุฉ ุนู ุฃูููุฉ ุงูุจูุงูุงุช ูู ุงูุฑุนุงูุฉ ุงูุตุญูุฉ.

ุงูุจูุงูุงุช ูู ุนุจุงุฑุฉ ุนู ูุนูููุงุช ุชู ุฌูุนูุง ูุชุญููููุง ูุชูุณูุฑูุงุ ููู ูููุฉ ุฌุฏูุง ูู ุงูุฑุนุงูุฉ ุงูุตุญูุฉุ ุญูุซ ุชุณุงุนุฏ ุนูู ุชุญุณูู ุฌูุฏุฉ ุงูุฑุนุงูุฉ ุงูุตุญูุฉุ ูุชูููู ุงูุฃุฎุทุงุก ุงูุทุจูุฉุ ูุชูููุฑ ุงูููุช ูุงูุฌูุฏุ ูุชุญุณูู ูุชุงุฆุฌ ุงูุนูุงุฌุ ูุชูููุฑ ุงููุงู.

ุนูู ุณุจูู ุงููุซุงูุ ูููู ุงุณุชุฎุฏุงู ุงูุจูุงูุงุช ูู ุงูุฑุนุงูุฉ ุงูุตุญูุฉ ูุชุญุฏูุฏ ุงููุฑุถู ุงูุฐูู ูุญุชุงุฌูู ุฅูู ุนูุงุฌ ูุญุฏุฏุ ูุชุญุฏูุฏ ุงูุนูุงุฌุงุช ุงูุฃูุซุฑ ูุนุงููุฉุ ูุชุญุฏูุฏ ุงูุนูุงูู ุงูุชู ุชุคุซุฑ ุนูู ุตุญุฉ ุงููุฑุถูุ ูุชุญุณูู ุฌูุฏุฉ ุงูุฑุนุงูุฉ ุงูุตุญูุฉุ ูุชุญุณูู ูุชุงุฆุฌ ุงูุนูุงุฌุ ูุชูููุฑ ุงููุงู.

ุจุงูุฅุถุงูุฉ ุฅูู ุฐููุ ูููู ุงุณุชุฎุฏุงู ุงูุจูุงูุงุช ูู ุงูุฑุนุงูุฉ ุงูุตุญูุฉ ูุชุญุณูู ุงูุชูุงุตู ุจูู ุงููุฑุถู ูุงูุฃุทุจุงุกุ ูุชุญุณูู ุฅุฏุงุฑุฉ ุงูุฑุนุงูุฉ ุงูุตุญูุฉุ ูุชุญุณูู ุชุฏุฑูุจ ุงูุนุงูููู ูู ูุฌุงู ุงูุฑุนุงูุฉ ุงูุตุญูุฉุ ูุชุญุณูู ุงูุจุญุซ ุงูุนููู ูู ูุฌุงู ุงูุฑุนุงูุฉ ุงูุตุญูุฉ.

ุจุงุฎุชุตุงุฑุ ูุฅู ุงูุจูุงูุงุช ูููุฉ ุฌุฏูุง ูู ุงูุฑุนุงูุฉ ุงูุตุญูุฉุ ุญูุซ ุชุณุงุนุฏ ุนูู ุชุญุณูู ุฌูุฏุฉ ุงูุฑุนุงูุฉ ุงูุตุญูุฉุ ูุชูููู ุงูุฃุฎุทุงุก ุงูุทุจูุฉุ ูุชูููุฑ ุงูููุช ูุงูุฌูุฏุ ูุชุญุณูู ูุชุงุฆุฌ ุงูุนูุงุฌุ ูุชูููุฑ ุงููุงู.

or with an English prompt

```bash
python3 jais_13b_inference_test.py \
  --model core42/jais-13b-chat \
  --prompt "Give me three bullet points on AMD ROCm for AI." \
  --hf_token YOUR_HF_TOKEN
```
=== Generated Text ===

Give me three bullet points on AMD ROCm for AI.

1.  ROCm is a multi-core architecture designed for AI workloads.
2.  ROCm includes a runtime that supports dynamic parallelism and fine-grained task scheduling.
3.  ROCm includes a compiler that supports auto-tuning and auto-vectorization.
